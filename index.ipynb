{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coding Logistic Regression From Scratch - Lab\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this lab, you'll practice your ability to translate mathematical algorithms into python functions. This will deepen and solidify your understanding of the logistic regression!\n",
    "\n",
    "## Objectives\n",
    "\n",
    "You will be able to:\n",
    "* Understand and implement logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "Recall that the logistic regression algorithm take our previous intuition from logistic regression. In logistic regression, we start by taking our input data, X and multiplying it by a vector of weights for each of the individual features, which produces our output y. Afterwards we'll work on using an iterative approach via gradient descent to tune these weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression Setup\n",
    "\n",
    "Write a simple function `predict_y` that takes in a matrix `X` of observations and a vector of feature weights `w` and outputs a vector of predictions for the various observations.\n",
    "\n",
    "Recall that this is the sum of the product of each of the feature observations and their corresponding feature weights:  \n",
    "$ \\hat{y}_i = X_{i1} \\bullet w_1 + X_{i2} \\bullet w_2 + X_{i3} \\bullet w_3 + ... + X_{in} \\bullet w_n$\n",
    "\n",
    "Hint: think about which mathematical operation we previously discussed that will take a matrix (X) and multiply it by a vector of weights (w) to succinctly do this in a single operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.5, 3.5])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import doctest\n",
    "\n",
    "def predict_y(X, w):\n",
    "    '''Multiply two vectors (same size) element-wise:\n",
    "    >>> predict_y([1,2,3], [4,5,6])\n",
    "    array([ 4, 10, 18])\n",
    "    '''\n",
    "    return np.mean(np.multiply(X, w), axis=1)\n",
    "\n",
    "predict_y([[1, 2], [3, 4]], [1, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Sigmoid Function\n",
    "\n",
    "Recall that the sigmoid function is used to map our previous linear regression model to a range of 0 to 1, satisfying basic premises of probability. As a reminder, the sigmoid function is defined by:  \n",
    "$\\frac{1}{1+e^(-x)}$  \n",
    "Write this as a python function where x is the input and the function outputs the result of the sigmoid function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graphing the Sigmoid\n",
    "\n",
    "For good measure, let's do a brief investigation of your new function. Graph the output of your sigmoid function using 10,000 X values evenly spaced from -20 to 20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "X_s = np.linspace(-20, 20, 10000)\n",
    "Y_s = [sigmoid(x) for x in X_s]\n",
    "sns.lineplot(X_s, Y_s);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent with the Sigmoid Function\n",
    "\n",
    "Recall that gradient descent is a numerical method for finding a minimum to a cost function. In the case of logistic regression, we are looking to minimize the error between our model's predictions and the actual data labels. To do this, we first calculate an error vector based on the current model's feature weights. We then multiply the transpose of the training matrix itself by this error vector in order to obtain the gradient. Finally, we take the gradient, multiply it by our step size and add this to our current weight vector to update it. Below, write such a function. It will take 5 inputs:  \n",
    "* X\n",
    "* y\n",
    "* max_iterations\n",
    "* alpha (the step size)\n",
    "* initial_weights  \n",
    "By default, have your function set the initial_weights parameter to a vector where all feature weights are set to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "def grad_desc(X, y, max_iterations=10000, alpha=1/10000, initial_weights=None):\n",
    "    \"\"\"Be sure to set default behavior for the initial_weights parameter.\"\"\"\n",
    "    if initial_weights == None:\n",
    "        weights = np.ones(X.shape[1])\n",
    "    else:\n",
    "        weights = initial_weights\n",
    "        \n",
    "    #Create a for loop of iterations\n",
    "    for i in range(max_iterations):\n",
    "        print(weights)\n",
    "        \n",
    "        #Generate predictions using the current feature weights\n",
    "        y_preds = predict_y(sigmoid(X), weights)\n",
    "        print(' ')\n",
    "        print('Y predictions ____________')\n",
    "        print(y_preds)\n",
    "        \n",
    "        #Calculate an error vector based on these initial predictions and the correct labels\n",
    "        print(' ')\n",
    "        print('error ____________')\n",
    "        error_vector = y - y_preds\n",
    "        print(error_vector)\n",
    "        \n",
    "        #Calculate the gradient \n",
    "        #As we saw in the previous lab, calculating the gradient is often the most difficult task.\n",
    "        #Here, your are provided with the closed form solution for the gradient of the log-loss function derived from MLE\n",
    "        #For more details on the derivation, see the additional resources section below.\n",
    "        gradient = np.dot(X.transpose(), error_vector) \n",
    "        print(' ')\n",
    "        print('gradient ____________')\n",
    "        print(gradient)\n",
    "        \n",
    "        #Update the weight vector take a step of alpha in direction of gradient\n",
    "        weights += gradient * alpha\n",
    "        \n",
    "    #Return finalized Weights\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running Your Algorithm\n",
    "\n",
    "Now that we've coded everything from the ground up, we can further investigate the convergence behavior of our gradient descent algorithm. Remember that gradient descent does not gaurantee a global minimum, only a local minimum, and that small deviations in the starting point or step size can lead to different outputs.  \n",
    "  \n",
    "Let's begin by running our algorithm and plotting the successive weights of the features through iterations. Below is a dataset, with X and y predefined for you. Use your logistic regression function to find train a model. As the model trains, record the iteration cycle of the gradient descent algorithm and the weights of the various features. Then, plot this data on subplots for each of the individual features. Each graph should have the iteration number on the x-axis and the value of that feature weight for that iteration cycle on the y-axis. This will visually display how the algorithm is adjusting the weights over successive iterations, and hopefully show convergence on stable weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0    165\n",
      "0.0    138\n",
      "Name: target, dtype: int64\n",
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " \n",
      "Y predictions ____________\n",
      "          age       sex        cp  trestbps      chol       fbs   restecg  \\\n",
      "0    0.670033  0.731059  0.731059  0.618015  0.560771  0.731059  0.500000   \n",
      "1    0.541570  0.731059  0.660756  0.584099  0.570307  0.500000  0.622459   \n",
      "2    0.562177  0.500000  0.582570  0.584099  0.544403  0.500000  0.500000   \n",
      "3    0.637031  0.731059  0.582570  0.561015  0.562457  0.500000  0.622459   \n",
      "4    0.641834  0.500000  0.500000  0.561015  0.627276  0.500000  0.622459   \n",
      "5    0.641834  0.731059  0.500000  0.606819  0.537600  0.500000  0.622459   \n",
      "6    0.637031  0.500000  0.582570  0.606819  0.594732  0.500000  0.500000   \n",
      "7    0.577495  0.731059  0.582570  0.561015  0.577565  0.500000  0.622459   \n",
      "8    0.617551  0.731059  0.660756  0.676087  0.541570  0.731059  0.622459   \n",
      "9    0.641834  0.731059  0.660756  0.629087  0.523954  0.500000  0.622459   \n",
      "10   0.627343  0.731059  0.500000  0.606819  0.564142  0.500000  0.622459   \n",
      "11   0.597686  0.500000  0.660756  0.584099  0.584235  0.500000  0.622459   \n",
      "12   0.602685  0.731059  0.582570  0.584099  0.579235  0.500000  0.622459   \n",
      "13   0.674622  0.731059  0.731059  0.537664  0.548364  0.500000  0.500000   \n",
      "14   0.646609  0.500000  0.731059  0.629087  0.588665  0.731059  0.500000   \n",
      "15   0.607663  0.500000  0.660756  0.561015  0.552884  0.500000  0.622459   \n",
      "16   0.646609  0.500000  0.660756  0.561015  0.619773  0.500000  0.622459   \n",
      "17   0.683701  0.500000  0.731059  0.629087  0.556831  0.500000  0.622459   \n",
      "18   0.572404  0.731059  0.500000  0.629087  0.568628  0.500000  0.622459   \n",
      "19   0.697059  0.500000  0.731059  0.606819  0.564142  0.500000  0.622459   \n",
      "20   0.651355  0.731059  0.500000  0.595510  0.561333  0.500000  0.622459   \n",
      "21   0.577495  0.731059  0.660756  0.584099  0.560771  0.500000  0.622459   \n",
      "22   0.567297  0.731059  0.500000  0.606819  0.556831  0.500000  0.622459   \n",
      "23   0.660756  0.731059  0.660756  0.629087  0.566387  0.731059  0.622459   \n",
      "24   0.557042  0.731059  0.731059  0.606819  0.541570  0.500000  0.622459   \n",
      "25   0.705785  0.500000  0.582570  0.650819  0.599126  0.500000  0.622459   \n",
      "26   0.651355  0.731059  0.660756  0.629087  0.548930  0.731059  0.622459   \n",
      "27   0.612619  0.731059  0.660756  0.537664  0.527939  0.500000  0.622459   \n",
      "28   0.679179  0.500000  0.660756  0.606819  0.660244  0.731059  0.500000   \n",
      "29   0.622459  0.731059  0.660756  0.584099  0.540437  0.731059  0.500000   \n",
      "..        ...       ...       ...       ...       ...       ...       ...   \n",
      "273  0.646609  0.731059  0.500000  0.514147  0.561333  0.500000  0.622459   \n",
      "274  0.592667  0.731059  0.500000  0.537664  0.584235  0.500000  0.500000   \n",
      "275  0.617551  0.731059  0.500000  0.572597  0.548930  0.500000  0.622459   \n",
      "276  0.646609  0.731059  0.500000  0.620240  0.552319  0.500000  0.622459   \n",
      "277  0.641834  0.731059  0.582570  0.570286  0.576451  0.500000  0.622459   \n",
      "278  0.646609  0.500000  0.582570  0.597781  0.608411  0.731059  0.500000   \n",
      "279  0.660756  0.731059  0.500000  0.602309  0.522815  0.500000  0.500000   \n",
      "280  0.567297  0.731059  0.500000  0.597781  0.606233  0.500000  0.622459   \n",
      "281  0.617551  0.731059  0.500000  0.579508  0.544403  0.731059  0.622459   \n",
      "282  0.651355  0.731059  0.660756  0.574904  0.552319  0.731059  0.622459   \n",
      "283  0.557042  0.731059  0.500000  0.633479  0.555140  0.500000  0.622459   \n",
      "284  0.660756  0.731059  0.500000  0.606819  0.546102  0.500000  0.500000   \n",
      "285  0.587628  0.731059  0.500000  0.606819  0.604051  0.500000  0.622459   \n",
      "286  0.651355  0.731059  0.731059  0.593236  0.544403  0.500000  0.622459   \n",
      "287  0.641834  0.731059  0.582570  0.637848  0.560209  0.500000  0.500000   \n",
      "288  0.641834  0.731059  0.500000  0.537664  0.617079  0.500000  0.622459   \n",
      "289  0.632200  0.500000  0.500000  0.579508  0.544969  0.500000  0.731059   \n",
      "290  0.660756  0.731059  0.500000  0.624674  0.543837  0.500000  0.622459   \n",
      "291  0.646609  0.731059  0.500000  0.547030  0.607867  0.500000  0.731059   \n",
      "292  0.646609  0.500000  0.500000  0.671942  0.556268  0.731059  0.500000   \n",
      "293  0.688189  0.731059  0.660756  0.633479  0.548930  0.500000  0.500000   \n",
      "294  0.577495  0.731059  0.500000  0.561015  0.524524  0.500000  0.622459   \n",
      "295  0.670033  0.731059  0.500000  0.606819  0.534761  0.500000  0.500000   \n",
      "296  0.670033  0.500000  0.500000  0.570286  0.540437  0.500000  0.622459   \n",
      "297  0.651355  0.731059  0.500000  0.659345  0.528508  0.731059  0.500000   \n",
      "298  0.641834  0.500000  0.500000  0.606819  0.565265  0.500000  0.622459   \n",
      "299  0.582570  0.731059  0.731059  0.537664  0.578122  0.500000  0.622459   \n",
      "300  0.692642  0.731059  0.500000  0.615786  0.538168  0.731059  0.622459   \n",
      "301  0.641834  0.731059  0.500000  0.584099  0.502854  0.500000  0.622459   \n",
      "302  0.641834  0.500000  0.582570  0.584099  0.562457  0.500000  0.500000   \n",
      "\n",
      "      thalach     exang   oldpeak     slope        ca      thal  \n",
      "0    0.646355  0.500000  0.591693  0.500000  0.500000  0.582570  \n",
      "1    0.707960  0.500000  0.637497  0.500000  0.500000  0.660756  \n",
      "2    0.683736  0.500000  0.556213  0.731059  0.500000  0.660756  \n",
      "3    0.693555  0.500000  0.532213  0.731059  0.500000  0.660756  \n",
      "4    0.668695  0.731059  0.524175  0.731059  0.500000  0.660756  \n",
      "5    0.642857  0.500000  0.516123  0.622459  0.500000  0.582570  \n",
      "6    0.651572  0.500000  0.552228  0.622459  0.500000  0.660756  \n",
      "7    0.685384  0.500000  0.500000  0.731059  0.500000  0.731059  \n",
      "8    0.667002  0.500000  0.520150  0.731059  0.500000  0.731059  \n",
      "9    0.687028  0.500000  0.564160  0.731059  0.500000  0.660756  \n",
      "10   0.663602  0.500000  0.548237  0.731059  0.500000  0.660756  \n",
      "11   0.626934  0.500000  0.508064  0.731059  0.500000  0.660756  \n",
      "12   0.682083  0.500000  0.524175  0.731059  0.500000  0.660756  \n",
      "13   0.635816  0.731059  0.572075  0.622459  0.500000  0.660756  \n",
      "14   0.667002  0.500000  0.540235  0.731059  0.500000  0.660756  \n",
      "15   0.660186  0.500000  0.564160  0.622459  0.500000  0.660756  \n",
      "16   0.683736  0.500000  0.500000  0.731059  0.500000  0.660756  \n",
      "17   0.581332  0.500000  0.603329  0.500000  0.500000  0.660756  \n",
      "18   0.682083  0.500000  0.560191  0.731059  0.500000  0.660756  \n",
      "19   0.648098  0.500000  0.572075  0.731059  0.622459  0.660756  \n",
      "20   0.665304  0.500000  0.520150  0.622459  0.500000  0.731059  \n",
      "21   0.695175  0.731059  0.516123  0.731059  0.500000  0.660756  \n",
      "22   0.693555  0.500000  0.500000  0.731059  0.500000  0.660756  \n",
      "23   0.623356  0.731059  0.540235  0.622459  0.500000  0.660756  \n",
      "24   0.693555  0.731059  0.556213  0.731059  0.500000  0.731059  \n",
      "25   0.667002  0.500000  0.516123  0.731059  0.622459  0.660756  \n",
      "26   0.658471  0.500000  0.564160  0.731059  0.500000  0.660756  \n",
      "27   0.597954  0.500000  0.524175  0.731059  0.500000  0.660756  \n",
      "28   0.658471  0.500000  0.532213  0.731059  0.562177  0.660756  \n",
      "29   0.649837  0.500000  0.548237  0.500000  0.500000  0.660756  \n",
      "..        ...       ...       ...       ...       ...       ...  \n",
      "273  0.656752  0.500000  0.504032  0.731059  0.562177  0.731059  \n",
      "274  0.588745  0.731059  0.540235  0.622459  0.562177  0.660756  \n",
      "275  0.677096  0.500000  0.540235  0.731059  0.622459  0.731059  \n",
      "276  0.564524  0.500000  0.579953  0.622459  0.562177  0.731059  \n",
      "277  0.630497  0.500000  0.512094  0.731059  0.500000  0.731059  \n",
      "278  0.649837  0.500000  0.500000  0.731059  0.622459  0.660756  \n",
      "279  0.601619  0.731059  0.641216  0.622459  0.562177  0.660756  \n",
      "280  0.601619  0.731059  0.572075  0.622459  0.500000  0.582570  \n",
      "281  0.656752  0.731059  0.540235  0.622459  0.500000  0.500000  \n",
      "282  0.617964  0.500000  0.587790  0.622459  0.562177  0.582570  \n",
      "283  0.698401  0.500000  0.500000  0.731059  0.500000  0.731059  \n",
      "284  0.625146  0.731059  0.576019  0.731059  0.562177  0.731059  \n",
      "285  0.592436  0.731059  0.572075  0.622459  0.622459  0.731059  \n",
      "286  0.667002  0.500000  0.532213  0.731059  0.622459  0.660756  \n",
      "287  0.670384  0.500000  0.500000  0.731059  0.562177  0.660756  \n",
      "288  0.634047  0.731059  0.618662  0.622459  0.562177  0.731059  \n",
      "289  0.610730  0.731059  0.579953  0.622459  0.562177  0.731059  \n",
      "290  0.665304  0.500000  0.500000  0.731059  0.562177  0.731059  \n",
      "291  0.628717  0.500000  0.670330  0.500000  0.679179  0.582570  \n",
      "292  0.639344  0.731059  0.611023  0.622459  0.622459  0.582570  \n",
      "293  0.646355  0.500000  0.532213  0.622459  0.500000  0.731059  \n",
      "294  0.635816  0.731059  0.611023  0.500000  0.500000  0.582570  \n",
      "295  0.635816  0.731059  0.655919  0.731059  0.622459  0.731059  \n",
      "296  0.621562  0.731059  0.500000  0.622459  0.500000  0.660756  \n",
      "297  0.536196  0.500000  0.540235  0.622459  0.622459  0.582570  \n",
      "298  0.597954  0.731059  0.508064  0.622459  0.500000  0.731059  \n",
      "299  0.614353  0.500000  0.548237  0.622459  0.500000  0.731059  \n",
      "300  0.630497  0.500000  0.633761  0.622459  0.622459  0.731059  \n",
      "301  0.583189  0.731059  0.548237  0.622459  0.562177  0.731059  \n",
      "302  0.687028  0.500000  0.500000  0.622459  0.562177  0.660756  \n",
      "\n",
      "[303 rows x 13 columns]\n",
      " \n",
      "error ____________\n",
      "     age  sex  cp  trestbps  chol  fbs  restecg  thalach  exang  oldpeak ...   \\\n",
      "0    NaN  NaN NaN       NaN   NaN  NaN      NaN      NaN    NaN      NaN ...    \n",
      "1    NaN  NaN NaN       NaN   NaN  NaN      NaN      NaN    NaN      NaN ...    \n",
      "2    NaN  NaN NaN       NaN   NaN  NaN      NaN      NaN    NaN      NaN ...    \n",
      "3    NaN  NaN NaN       NaN   NaN  NaN      NaN      NaN    NaN      NaN ...    \n",
      "4    NaN  NaN NaN       NaN   NaN  NaN      NaN      NaN    NaN      NaN ...    \n",
      "5    NaN  NaN NaN       NaN   NaN  NaN      NaN      NaN    NaN      NaN ...    \n",
      "6    NaN  NaN NaN       NaN   NaN  NaN      NaN      NaN    NaN      NaN ...    \n",
      "7    NaN  NaN NaN       NaN   NaN  NaN      NaN      NaN    NaN      NaN ...    \n",
      "8    NaN  NaN NaN       NaN   NaN  NaN      NaN      NaN    NaN      NaN ...    \n",
      "9    NaN  NaN NaN       NaN   NaN  NaN      NaN      NaN    NaN      NaN ...    \n",
      "10   NaN  NaN NaN       NaN   NaN  NaN      NaN      NaN    NaN      NaN ...    \n",
      "11   NaN  NaN NaN       NaN   NaN  NaN      NaN      NaN    NaN      NaN ...    \n",
      "12   NaN  NaN NaN       NaN   NaN  NaN      NaN      NaN    NaN      NaN ...    \n",
      "13   NaN  NaN NaN       NaN   NaN  NaN      NaN      NaN    NaN      NaN ...    \n",
      "14   NaN  NaN NaN       NaN   NaN  NaN      NaN      NaN    NaN      NaN ...    \n",
      "15   NaN  NaN NaN       NaN   NaN  NaN      NaN      NaN    NaN      NaN ...    \n",
      "16   NaN  NaN NaN       NaN   NaN  NaN      NaN      NaN    NaN      NaN ...    \n",
      "17   NaN  NaN NaN       NaN   NaN  NaN      NaN      NaN    NaN      NaN ...    \n",
      "18   NaN  NaN NaN       NaN   NaN  NaN      NaN      NaN    NaN      NaN ...    \n",
      "19   NaN  NaN NaN       NaN   NaN  NaN      NaN      NaN    NaN      NaN ...    \n",
      "20   NaN  NaN NaN       NaN   NaN  NaN      NaN      NaN    NaN      NaN ...    \n",
      "21   NaN  NaN NaN       NaN   NaN  NaN      NaN      NaN    NaN      NaN ...    \n",
      "22   NaN  NaN NaN       NaN   NaN  NaN      NaN      NaN    NaN      NaN ...    \n",
      "23   NaN  NaN NaN       NaN   NaN  NaN      NaN      NaN    NaN      NaN ...    \n",
      "24   NaN  NaN NaN       NaN   NaN  NaN      NaN      NaN    NaN      NaN ...    \n",
      "25   NaN  NaN NaN       NaN   NaN  NaN      NaN      NaN    NaN      NaN ...    \n",
      "26   NaN  NaN NaN       NaN   NaN  NaN      NaN      NaN    NaN      NaN ...    \n",
      "27   NaN  NaN NaN       NaN   NaN  NaN      NaN      NaN    NaN      NaN ...    \n",
      "28   NaN  NaN NaN       NaN   NaN  NaN      NaN      NaN    NaN      NaN ...    \n",
      "29   NaN  NaN NaN       NaN   NaN  NaN      NaN      NaN    NaN      NaN ...    \n",
      "..   ...  ...  ..       ...   ...  ...      ...      ...    ...      ... ...    \n",
      "273  NaN  NaN NaN       NaN   NaN  NaN      NaN      NaN    NaN      NaN ...    \n",
      "274  NaN  NaN NaN       NaN   NaN  NaN      NaN      NaN    NaN      NaN ...    \n",
      "275  NaN  NaN NaN       NaN   NaN  NaN      NaN      NaN    NaN      NaN ...    \n",
      "276  NaN  NaN NaN       NaN   NaN  NaN      NaN      NaN    NaN      NaN ...    \n",
      "277  NaN  NaN NaN       NaN   NaN  NaN      NaN      NaN    NaN      NaN ...    \n",
      "278  NaN  NaN NaN       NaN   NaN  NaN      NaN      NaN    NaN      NaN ...    \n",
      "279  NaN  NaN NaN       NaN   NaN  NaN      NaN      NaN    NaN      NaN ...    \n",
      "280  NaN  NaN NaN       NaN   NaN  NaN      NaN      NaN    NaN      NaN ...    \n",
      "281  NaN  NaN NaN       NaN   NaN  NaN      NaN      NaN    NaN      NaN ...    \n",
      "282  NaN  NaN NaN       NaN   NaN  NaN      NaN      NaN    NaN      NaN ...    \n",
      "283  NaN  NaN NaN       NaN   NaN  NaN      NaN      NaN    NaN      NaN ...    \n",
      "284  NaN  NaN NaN       NaN   NaN  NaN      NaN      NaN    NaN      NaN ...    \n",
      "285  NaN  NaN NaN       NaN   NaN  NaN      NaN      NaN    NaN      NaN ...    \n",
      "286  NaN  NaN NaN       NaN   NaN  NaN      NaN      NaN    NaN      NaN ...    \n",
      "287  NaN  NaN NaN       NaN   NaN  NaN      NaN      NaN    NaN      NaN ...    \n",
      "288  NaN  NaN NaN       NaN   NaN  NaN      NaN      NaN    NaN      NaN ...    \n",
      "289  NaN  NaN NaN       NaN   NaN  NaN      NaN      NaN    NaN      NaN ...    \n",
      "290  NaN  NaN NaN       NaN   NaN  NaN      NaN      NaN    NaN      NaN ...    \n",
      "291  NaN  NaN NaN       NaN   NaN  NaN      NaN      NaN    NaN      NaN ...    \n",
      "292  NaN  NaN NaN       NaN   NaN  NaN      NaN      NaN    NaN      NaN ...    \n",
      "293  NaN  NaN NaN       NaN   NaN  NaN      NaN      NaN    NaN      NaN ...    \n",
      "294  NaN  NaN NaN       NaN   NaN  NaN      NaN      NaN    NaN      NaN ...    \n",
      "295  NaN  NaN NaN       NaN   NaN  NaN      NaN      NaN    NaN      NaN ...    \n",
      "296  NaN  NaN NaN       NaN   NaN  NaN      NaN      NaN    NaN      NaN ...    \n",
      "297  NaN  NaN NaN       NaN   NaN  NaN      NaN      NaN    NaN      NaN ...    \n",
      "298  NaN  NaN NaN       NaN   NaN  NaN      NaN      NaN    NaN      NaN ...    \n",
      "299  NaN  NaN NaN       NaN   NaN  NaN      NaN      NaN    NaN      NaN ...    \n",
      "300  NaN  NaN NaN       NaN   NaN  NaN      NaN      NaN    NaN      NaN ...    \n",
      "301  NaN  NaN NaN       NaN   NaN  NaN      NaN      NaN    NaN      NaN ...    \n",
      "302  NaN  NaN NaN       NaN   NaN  NaN      NaN      NaN    NaN      NaN ...    \n",
      "\n",
      "     293  294  295  296  297  298  299  300  301  302  \n",
      "0    NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
      "1    NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
      "2    NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
      "3    NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
      "4    NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
      "5    NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
      "6    NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
      "7    NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
      "8    NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
      "9    NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
      "10   NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
      "11   NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
      "12   NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
      "13   NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
      "14   NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
      "15   NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
      "16   NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
      "17   NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
      "18   NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
      "19   NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
      "20   NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
      "21   NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
      "22   NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
      "23   NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
      "24   NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
      "25   NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
      "26   NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
      "27   NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
      "28   NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
      "29   NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
      "..   ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  \n",
      "273  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
      "274  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
      "275  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
      "276  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
      "277  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
      "278  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
      "279  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
      "280  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
      "281  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
      "282  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
      "283  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
      "284  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
      "285  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
      "286  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
      "287  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
      "288  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
      "289  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
      "290  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
      "291  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
      "292  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
      "293  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
      "294  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
      "295  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
      "296  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
      "297  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
      "298  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
      "299  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
      "300  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
      "301  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
      "302  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
      "\n",
      "[303 rows x 316 columns]\n",
      " \n",
      "gradient ____________\n",
      "[[nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]\n",
      " ...\n",
      " [nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/learn-env/lib/python3.6/site-packages/pandas/core/indexes/base.py:3772: RuntimeWarning: '<' not supported between instances of 'str' and 'int', sort order is undefined for incomparable objects\n",
      "  return this.join(other, how=how, return_indexers=return_indexers)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (13,) (13,316) (13,) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mValueError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-54-c94419467214>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue_counts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mgrad_desc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-53-ca008ec2566d>\u001b[0m in \u001b[0;36mgrad_desc\u001b[0;34m(X, y, max_iterations, alpha, initial_weights)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;31m#Update the weight vector take a step of alpha in direction of gradient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0mweights\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mgradient\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;31m#Return finalized Weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (13,) (13,316) (13,) "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('heart.csv')\n",
    "\n",
    "X = df[df.columns[:-1]]\n",
    "y = df.target\n",
    "\n",
    "print(y.value_counts())\n",
    "X.head()\n",
    "grad_desc(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sci-kit learn\n",
    "\n",
    "For comparison, import sci-kit learn's standard LogisticRegression function. Initialize a regression object with **no intercept** and with **C=1e16** or another very high number. The reason is as follows: our implementation has not used an intercept, and we have not performed any regularization such as Lasso or Ridge (sci-kit learn uses l2 by default). The high value of C will essentially negate this.\n",
    "\n",
    "After initializing a regression object, fit it to X and y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare the models\n",
    "\n",
    "Compare the coefficient weights of your model to that generated by sci-kit learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Level - Up\n",
    "\n",
    "Update the gradient descent algorithm to also return the prediction error after each iteration. Then rerun the algorithm and create a graph displaying the prediction errors versus the iteration number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Resources\n",
    "\n",
    "If you want to see more of the mathematics behind the gradient derivation above, check out section 4.4.1 from the Elements of Statistical Learning which can be found here: https://web.stanford.edu/~hastie/ElemStatLearn//."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Congratulations! You just coded logistic regression from the ground up using NumPy! With this, you should have a fairly deep understanding of logistic regression and how the algorithm works! In the upcoming labs, we'll continue to explore this from a few more angles, plotting our data along with the decision boundary for our predictions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
