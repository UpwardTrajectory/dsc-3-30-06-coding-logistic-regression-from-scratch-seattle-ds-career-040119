{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coding Logistic Regression From Scratch - Lab\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this lab, you'll practice your ability to translate mathematical algorithms into python functions. This will deepen and solidify your understanding of the logistic regression!\n",
    "\n",
    "## Objectives\n",
    "\n",
    "You will be able to:\n",
    "* Understand and implement logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "Recall that the logistic regression algorithm take our previous intuition from logistic regression. In logistic regression, we start by taking our input data, X and multiplying it by a vector of weights for each of the individual features, which produces our output y. Afterwards we'll work on using an iterative approach via gradient descent to tune these weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression Setup\n",
    "\n",
    "Write a simple function `predict_y` that takes in a matrix `X` of observations and a vector of feature weights `w` and outputs a vector of predictions for the various observations.\n",
    "\n",
    "Recall that this is the sum of the product of each of the feature observations and their corresponding feature weights:  \n",
    "$ \\hat{y}_i = X_{i1} \\bullet w_1 + X_{i2} \\bullet w_2 + X_{i3} \\bullet w_3 + ... + X_{in} \\bullet w_n$\n",
    "\n",
    "Hint: think about which mathematical operation we previously discussed that will take a matrix (X) and multiply it by a vector of weights (w) to succinctly do this in a single operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.5, 3.5])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import doctest\n",
    "\n",
    "def predict_y(X, w):\n",
    "    '''Multiply two vectors (same size) element-wise:\n",
    "    >>> predict_y([1,2,3], [4,5,6])\n",
    "    array([ 4, 10, 18])\n",
    "    '''\n",
    "    return np.mean(np.multiply(X, w), axis=1)\n",
    "\n",
    "predict_y([[1, 2], [3, 4]], [1, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Sigmoid Function\n",
    "\n",
    "Recall that the sigmoid function is used to map our previous linear regression model to a range of 0 to 1, satisfying basic premises of probability. As a reminder, the sigmoid function is defined by:  \n",
    "$\\frac{1}{1+e^(-x)}$  \n",
    "Write this as a python function where x is the input and the function outputs the result of the sigmoid function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graphing the Sigmoid\n",
    "\n",
    "For good measure, let's do a brief investigation of your new function. Graph the output of your sigmoid function using 10,000 X values evenly spaced from -20 to 20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "X_s = np.linspace(-20, 20, 10000)\n",
    "Y_s = [sigmoid(x) for x in X_s]\n",
    "sns.lineplot(X_s, Y_s);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent with the Sigmoid Function\n",
    "\n",
    "Recall that gradient descent is a numerical method for finding a minimum to a cost function. In the case of logistic regression, we are looking to minimize the error between our model's predictions and the actual data labels. To do this, we first calculate an error vector based on the current model's feature weights. We then multiply the transpose of the training matrix itself by this error vector in order to obtain the gradient. Finally, we take the gradient, multiply it by our step size and add this to our current weight vector to update it. Below, write such a function. It will take 5 inputs:  \n",
    "* X\n",
    "* y\n",
    "* max_iterations\n",
    "* alpha (the step size)\n",
    "* initial_weights  \n",
    "By default, have your function set the initial_weights parameter to a vector where all feature weights are set to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "def grad_desc(X, y, max_iterations=10000, alpha=1/10000, initial_weights=None):\n",
    "    \"\"\"Be sure to set default behavior for the initial_weights parameter.\"\"\"\n",
    "    if initial_weights is not None:\n",
    "        temp_weights = initial_weights\n",
    "    else:\n",
    "        temp_weights = np.ones(X.shape[1])\n",
    "        \n",
    "    #Create a for loop of iterations\n",
    "    for i in range(max_iterations):\n",
    "        \n",
    "        #Generate predictions using the current feature weights\n",
    "        y_preds = predict_y(sigmoid(X), temp_weights)\n",
    "        \n",
    "        #Calculate an error vector based on these initial predictions and the correct labels\n",
    "        error_vector = y - y_preds\n",
    "        \n",
    "        #Calculate the gradient \n",
    "        #As we saw in the previous lab, calculating the gradient is often the most difficult task.\n",
    "        #Here, your are provided with the closed form solution for the gradient of the log-loss function derived from MLE\n",
    "        #For more details on the derivation, see the additional resources section below.\n",
    "        gradient = np.dot(X.transpose(), error_vector) \n",
    "        \n",
    "        #Update the weight vector take a step of alpha in direction of gradient\n",
    "        temp_weights += gradient * alpha\n",
    "        \n",
    "    #Return finalized Weights\n",
    "    return temp_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running Your Algorithm\n",
    "\n",
    "Now that we've coded everything from the ground up, we can further investigate the convergence behavior of our gradient descent algorithm. Remember that gradient descent does not gaurantee a global minimum, only a local minimum, and that small deviations in the starting point or step size can lead to different outputs.  \n",
    "  \n",
    "Let's begin by running our algorithm and plotting the successive weights of the features through iterations. Below is a dataset, with X and y predefined for you. Use your logistic regression function to find train a model. As the model trains, record the iteration cycle of the gradient descent algorithm and the weights of the various features. Then, plot this data on subplots for each of the individual features. Each graph should have the iteration number on the x-axis and the value of that feature weight for that iteration cycle on the y-axis. This will visually display how the algorithm is adjusting the weights over successive iterations, and hopefully show convergence on stable weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "type numpy.ndarray doesn't define __round__ method",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-1ea4642976aa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mweights_over_time\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecent_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m200\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecent_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mweights_over_time\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: type numpy.ndarray doesn't define __round__ method"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('heart.csv')\n",
    "\n",
    "X = df[df.columns[:-1]]\n",
    "y = df.target\n",
    "\n",
    "weights_over_time = [np.ones(X.shape[1])]\n",
    "print(weights_over_time)\n",
    "\n",
    "for n in range(2000):\n",
    "    recent_weights = grad_desc(X, y, max_iterations=5, initial_weights=weights_over_time[-1])\n",
    "    weights_over_time.append(recent_weights)\n",
    "    if n % 200 == 0:\n",
    "        print(round(recent_weights)\n",
    "                             \n",
    "weights_over_time[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.575926</td>\n",
       "      <td>-7.211844</td>\n",
       "      <td>14.708444</td>\n",
       "      <td>-0.050287</td>\n",
       "      <td>1.231463</td>\n",
       "      <td>0.435285</td>\n",
       "      <td>5.35168</td>\n",
       "      <td>10.891106</td>\n",
       "      <td>-11.571547</td>\n",
       "      <td>-5.697492</td>\n",
       "      <td>13.147513</td>\n",
       "      <td>-8.857483</td>\n",
       "      <td>-2.365073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.575926</td>\n",
       "      <td>-7.211844</td>\n",
       "      <td>14.708444</td>\n",
       "      <td>-0.050287</td>\n",
       "      <td>1.231463</td>\n",
       "      <td>0.435285</td>\n",
       "      <td>5.35168</td>\n",
       "      <td>10.891106</td>\n",
       "      <td>-11.571547</td>\n",
       "      <td>-5.697492</td>\n",
       "      <td>13.147513</td>\n",
       "      <td>-8.857483</td>\n",
       "      <td>-2.365073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.575926</td>\n",
       "      <td>-7.211844</td>\n",
       "      <td>14.708444</td>\n",
       "      <td>-0.050287</td>\n",
       "      <td>1.231463</td>\n",
       "      <td>0.435285</td>\n",
       "      <td>5.35168</td>\n",
       "      <td>10.891106</td>\n",
       "      <td>-11.571547</td>\n",
       "      <td>-5.697492</td>\n",
       "      <td>13.147513</td>\n",
       "      <td>-8.857483</td>\n",
       "      <td>-2.365073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.575926</td>\n",
       "      <td>-7.211844</td>\n",
       "      <td>14.708444</td>\n",
       "      <td>-0.050287</td>\n",
       "      <td>1.231463</td>\n",
       "      <td>0.435285</td>\n",
       "      <td>5.35168</td>\n",
       "      <td>10.891106</td>\n",
       "      <td>-11.571547</td>\n",
       "      <td>-5.697492</td>\n",
       "      <td>13.147513</td>\n",
       "      <td>-8.857483</td>\n",
       "      <td>-2.365073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.575926</td>\n",
       "      <td>-7.211844</td>\n",
       "      <td>14.708444</td>\n",
       "      <td>-0.050287</td>\n",
       "      <td>1.231463</td>\n",
       "      <td>0.435285</td>\n",
       "      <td>5.35168</td>\n",
       "      <td>10.891106</td>\n",
       "      <td>-11.571547</td>\n",
       "      <td>-5.697492</td>\n",
       "      <td>13.147513</td>\n",
       "      <td>-8.857483</td>\n",
       "      <td>-2.365073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.575926</td>\n",
       "      <td>-7.211844</td>\n",
       "      <td>14.708444</td>\n",
       "      <td>-0.050287</td>\n",
       "      <td>1.231463</td>\n",
       "      <td>0.435285</td>\n",
       "      <td>5.35168</td>\n",
       "      <td>10.891106</td>\n",
       "      <td>-11.571547</td>\n",
       "      <td>-5.697492</td>\n",
       "      <td>13.147513</td>\n",
       "      <td>-8.857483</td>\n",
       "      <td>-2.365073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-0.575926</td>\n",
       "      <td>-7.211844</td>\n",
       "      <td>14.708444</td>\n",
       "      <td>-0.050287</td>\n",
       "      <td>1.231463</td>\n",
       "      <td>0.435285</td>\n",
       "      <td>5.35168</td>\n",
       "      <td>10.891106</td>\n",
       "      <td>-11.571547</td>\n",
       "      <td>-5.697492</td>\n",
       "      <td>13.147513</td>\n",
       "      <td>-8.857483</td>\n",
       "      <td>-2.365073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-0.575926</td>\n",
       "      <td>-7.211844</td>\n",
       "      <td>14.708444</td>\n",
       "      <td>-0.050287</td>\n",
       "      <td>1.231463</td>\n",
       "      <td>0.435285</td>\n",
       "      <td>5.35168</td>\n",
       "      <td>10.891106</td>\n",
       "      <td>-11.571547</td>\n",
       "      <td>-5.697492</td>\n",
       "      <td>13.147513</td>\n",
       "      <td>-8.857483</td>\n",
       "      <td>-2.365073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-0.575926</td>\n",
       "      <td>-7.211844</td>\n",
       "      <td>14.708444</td>\n",
       "      <td>-0.050287</td>\n",
       "      <td>1.231463</td>\n",
       "      <td>0.435285</td>\n",
       "      <td>5.35168</td>\n",
       "      <td>10.891106</td>\n",
       "      <td>-11.571547</td>\n",
       "      <td>-5.697492</td>\n",
       "      <td>13.147513</td>\n",
       "      <td>-8.857483</td>\n",
       "      <td>-2.365073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-0.575926</td>\n",
       "      <td>-7.211844</td>\n",
       "      <td>14.708444</td>\n",
       "      <td>-0.050287</td>\n",
       "      <td>1.231463</td>\n",
       "      <td>0.435285</td>\n",
       "      <td>5.35168</td>\n",
       "      <td>10.891106</td>\n",
       "      <td>-11.571547</td>\n",
       "      <td>-5.697492</td>\n",
       "      <td>13.147513</td>\n",
       "      <td>-8.857483</td>\n",
       "      <td>-2.365073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-0.575926</td>\n",
       "      <td>-7.211844</td>\n",
       "      <td>14.708444</td>\n",
       "      <td>-0.050287</td>\n",
       "      <td>1.231463</td>\n",
       "      <td>0.435285</td>\n",
       "      <td>5.35168</td>\n",
       "      <td>10.891106</td>\n",
       "      <td>-11.571547</td>\n",
       "      <td>-5.697492</td>\n",
       "      <td>13.147513</td>\n",
       "      <td>-8.857483</td>\n",
       "      <td>-2.365073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>-0.575926</td>\n",
       "      <td>-7.211844</td>\n",
       "      <td>14.708444</td>\n",
       "      <td>-0.050287</td>\n",
       "      <td>1.231463</td>\n",
       "      <td>0.435285</td>\n",
       "      <td>5.35168</td>\n",
       "      <td>10.891106</td>\n",
       "      <td>-11.571547</td>\n",
       "      <td>-5.697492</td>\n",
       "      <td>13.147513</td>\n",
       "      <td>-8.857483</td>\n",
       "      <td>-2.365073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>-0.575926</td>\n",
       "      <td>-7.211844</td>\n",
       "      <td>14.708444</td>\n",
       "      <td>-0.050287</td>\n",
       "      <td>1.231463</td>\n",
       "      <td>0.435285</td>\n",
       "      <td>5.35168</td>\n",
       "      <td>10.891106</td>\n",
       "      <td>-11.571547</td>\n",
       "      <td>-5.697492</td>\n",
       "      <td>13.147513</td>\n",
       "      <td>-8.857483</td>\n",
       "      <td>-2.365073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-0.575926</td>\n",
       "      <td>-7.211844</td>\n",
       "      <td>14.708444</td>\n",
       "      <td>-0.050287</td>\n",
       "      <td>1.231463</td>\n",
       "      <td>0.435285</td>\n",
       "      <td>5.35168</td>\n",
       "      <td>10.891106</td>\n",
       "      <td>-11.571547</td>\n",
       "      <td>-5.697492</td>\n",
       "      <td>13.147513</td>\n",
       "      <td>-8.857483</td>\n",
       "      <td>-2.365073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-0.575926</td>\n",
       "      <td>-7.211844</td>\n",
       "      <td>14.708444</td>\n",
       "      <td>-0.050287</td>\n",
       "      <td>1.231463</td>\n",
       "      <td>0.435285</td>\n",
       "      <td>5.35168</td>\n",
       "      <td>10.891106</td>\n",
       "      <td>-11.571547</td>\n",
       "      <td>-5.697492</td>\n",
       "      <td>13.147513</td>\n",
       "      <td>-8.857483</td>\n",
       "      <td>-2.365073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>-0.575926</td>\n",
       "      <td>-7.211844</td>\n",
       "      <td>14.708444</td>\n",
       "      <td>-0.050287</td>\n",
       "      <td>1.231463</td>\n",
       "      <td>0.435285</td>\n",
       "      <td>5.35168</td>\n",
       "      <td>10.891106</td>\n",
       "      <td>-11.571547</td>\n",
       "      <td>-5.697492</td>\n",
       "      <td>13.147513</td>\n",
       "      <td>-8.857483</td>\n",
       "      <td>-2.365073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>-0.575926</td>\n",
       "      <td>-7.211844</td>\n",
       "      <td>14.708444</td>\n",
       "      <td>-0.050287</td>\n",
       "      <td>1.231463</td>\n",
       "      <td>0.435285</td>\n",
       "      <td>5.35168</td>\n",
       "      <td>10.891106</td>\n",
       "      <td>-11.571547</td>\n",
       "      <td>-5.697492</td>\n",
       "      <td>13.147513</td>\n",
       "      <td>-8.857483</td>\n",
       "      <td>-2.365073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>-0.575926</td>\n",
       "      <td>-7.211844</td>\n",
       "      <td>14.708444</td>\n",
       "      <td>-0.050287</td>\n",
       "      <td>1.231463</td>\n",
       "      <td>0.435285</td>\n",
       "      <td>5.35168</td>\n",
       "      <td>10.891106</td>\n",
       "      <td>-11.571547</td>\n",
       "      <td>-5.697492</td>\n",
       "      <td>13.147513</td>\n",
       "      <td>-8.857483</td>\n",
       "      <td>-2.365073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>-0.575926</td>\n",
       "      <td>-7.211844</td>\n",
       "      <td>14.708444</td>\n",
       "      <td>-0.050287</td>\n",
       "      <td>1.231463</td>\n",
       "      <td>0.435285</td>\n",
       "      <td>5.35168</td>\n",
       "      <td>10.891106</td>\n",
       "      <td>-11.571547</td>\n",
       "      <td>-5.697492</td>\n",
       "      <td>13.147513</td>\n",
       "      <td>-8.857483</td>\n",
       "      <td>-2.365073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>-0.575926</td>\n",
       "      <td>-7.211844</td>\n",
       "      <td>14.708444</td>\n",
       "      <td>-0.050287</td>\n",
       "      <td>1.231463</td>\n",
       "      <td>0.435285</td>\n",
       "      <td>5.35168</td>\n",
       "      <td>10.891106</td>\n",
       "      <td>-11.571547</td>\n",
       "      <td>-5.697492</td>\n",
       "      <td>13.147513</td>\n",
       "      <td>-8.857483</td>\n",
       "      <td>-2.365073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>-0.575926</td>\n",
       "      <td>-7.211844</td>\n",
       "      <td>14.708444</td>\n",
       "      <td>-0.050287</td>\n",
       "      <td>1.231463</td>\n",
       "      <td>0.435285</td>\n",
       "      <td>5.35168</td>\n",
       "      <td>10.891106</td>\n",
       "      <td>-11.571547</td>\n",
       "      <td>-5.697492</td>\n",
       "      <td>13.147513</td>\n",
       "      <td>-8.857483</td>\n",
       "      <td>-2.365073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>-0.575926</td>\n",
       "      <td>-7.211844</td>\n",
       "      <td>14.708444</td>\n",
       "      <td>-0.050287</td>\n",
       "      <td>1.231463</td>\n",
       "      <td>0.435285</td>\n",
       "      <td>5.35168</td>\n",
       "      <td>10.891106</td>\n",
       "      <td>-11.571547</td>\n",
       "      <td>-5.697492</td>\n",
       "      <td>13.147513</td>\n",
       "      <td>-8.857483</td>\n",
       "      <td>-2.365073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>-0.575926</td>\n",
       "      <td>-7.211844</td>\n",
       "      <td>14.708444</td>\n",
       "      <td>-0.050287</td>\n",
       "      <td>1.231463</td>\n",
       "      <td>0.435285</td>\n",
       "      <td>5.35168</td>\n",
       "      <td>10.891106</td>\n",
       "      <td>-11.571547</td>\n",
       "      <td>-5.697492</td>\n",
       "      <td>13.147513</td>\n",
       "      <td>-8.857483</td>\n",
       "      <td>-2.365073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>-0.575926</td>\n",
       "      <td>-7.211844</td>\n",
       "      <td>14.708444</td>\n",
       "      <td>-0.050287</td>\n",
       "      <td>1.231463</td>\n",
       "      <td>0.435285</td>\n",
       "      <td>5.35168</td>\n",
       "      <td>10.891106</td>\n",
       "      <td>-11.571547</td>\n",
       "      <td>-5.697492</td>\n",
       "      <td>13.147513</td>\n",
       "      <td>-8.857483</td>\n",
       "      <td>-2.365073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>-0.575926</td>\n",
       "      <td>-7.211844</td>\n",
       "      <td>14.708444</td>\n",
       "      <td>-0.050287</td>\n",
       "      <td>1.231463</td>\n",
       "      <td>0.435285</td>\n",
       "      <td>5.35168</td>\n",
       "      <td>10.891106</td>\n",
       "      <td>-11.571547</td>\n",
       "      <td>-5.697492</td>\n",
       "      <td>13.147513</td>\n",
       "      <td>-8.857483</td>\n",
       "      <td>-2.365073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>-0.575926</td>\n",
       "      <td>-7.211844</td>\n",
       "      <td>14.708444</td>\n",
       "      <td>-0.050287</td>\n",
       "      <td>1.231463</td>\n",
       "      <td>0.435285</td>\n",
       "      <td>5.35168</td>\n",
       "      <td>10.891106</td>\n",
       "      <td>-11.571547</td>\n",
       "      <td>-5.697492</td>\n",
       "      <td>13.147513</td>\n",
       "      <td>-8.857483</td>\n",
       "      <td>-2.365073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>-0.575926</td>\n",
       "      <td>-7.211844</td>\n",
       "      <td>14.708444</td>\n",
       "      <td>-0.050287</td>\n",
       "      <td>1.231463</td>\n",
       "      <td>0.435285</td>\n",
       "      <td>5.35168</td>\n",
       "      <td>10.891106</td>\n",
       "      <td>-11.571547</td>\n",
       "      <td>-5.697492</td>\n",
       "      <td>13.147513</td>\n",
       "      <td>-8.857483</td>\n",
       "      <td>-2.365073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>-0.575926</td>\n",
       "      <td>-7.211844</td>\n",
       "      <td>14.708444</td>\n",
       "      <td>-0.050287</td>\n",
       "      <td>1.231463</td>\n",
       "      <td>0.435285</td>\n",
       "      <td>5.35168</td>\n",
       "      <td>10.891106</td>\n",
       "      <td>-11.571547</td>\n",
       "      <td>-5.697492</td>\n",
       "      <td>13.147513</td>\n",
       "      <td>-8.857483</td>\n",
       "      <td>-2.365073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>-0.575926</td>\n",
       "      <td>-7.211844</td>\n",
       "      <td>14.708444</td>\n",
       "      <td>-0.050287</td>\n",
       "      <td>1.231463</td>\n",
       "      <td>0.435285</td>\n",
       "      <td>5.35168</td>\n",
       "      <td>10.891106</td>\n",
       "      <td>-11.571547</td>\n",
       "      <td>-5.697492</td>\n",
       "      <td>13.147513</td>\n",
       "      <td>-8.857483</td>\n",
       "      <td>-2.365073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>-0.575926</td>\n",
       "      <td>-7.211844</td>\n",
       "      <td>14.708444</td>\n",
       "      <td>-0.050287</td>\n",
       "      <td>1.231463</td>\n",
       "      <td>0.435285</td>\n",
       "      <td>5.35168</td>\n",
       "      <td>10.891106</td>\n",
       "      <td>-11.571547</td>\n",
       "      <td>-5.697492</td>\n",
       "      <td>13.147513</td>\n",
       "      <td>-8.857483</td>\n",
       "      <td>-2.365073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1971</th>\n",
       "      <td>-0.575926</td>\n",
       "      <td>-7.211844</td>\n",
       "      <td>14.708444</td>\n",
       "      <td>-0.050287</td>\n",
       "      <td>1.231463</td>\n",
       "      <td>0.435285</td>\n",
       "      <td>5.35168</td>\n",
       "      <td>10.891106</td>\n",
       "      <td>-11.571547</td>\n",
       "      <td>-5.697492</td>\n",
       "      <td>13.147513</td>\n",
       "      <td>-8.857483</td>\n",
       "      <td>-2.365073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1972</th>\n",
       "      <td>-0.575926</td>\n",
       "      <td>-7.211844</td>\n",
       "      <td>14.708444</td>\n",
       "      <td>-0.050287</td>\n",
       "      <td>1.231463</td>\n",
       "      <td>0.435285</td>\n",
       "      <td>5.35168</td>\n",
       "      <td>10.891106</td>\n",
       "      <td>-11.571547</td>\n",
       "      <td>-5.697492</td>\n",
       "      <td>13.147513</td>\n",
       "      <td>-8.857483</td>\n",
       "      <td>-2.365073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1973</th>\n",
       "      <td>-0.575926</td>\n",
       "      <td>-7.211844</td>\n",
       "      <td>14.708444</td>\n",
       "      <td>-0.050287</td>\n",
       "      <td>1.231463</td>\n",
       "      <td>0.435285</td>\n",
       "      <td>5.35168</td>\n",
       "      <td>10.891106</td>\n",
       "      <td>-11.571547</td>\n",
       "      <td>-5.697492</td>\n",
       "      <td>13.147513</td>\n",
       "      <td>-8.857483</td>\n",
       "      <td>-2.365073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1974</th>\n",
       "      <td>-0.575926</td>\n",
       "      <td>-7.211844</td>\n",
       "      <td>14.708444</td>\n",
       "      <td>-0.050287</td>\n",
       "      <td>1.231463</td>\n",
       "      <td>0.435285</td>\n",
       "      <td>5.35168</td>\n",
       "      <td>10.891106</td>\n",
       "      <td>-11.571547</td>\n",
       "      <td>-5.697492</td>\n",
       "      <td>13.147513</td>\n",
       "      <td>-8.857483</td>\n",
       "      <td>-2.365073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1975</th>\n",
       "      <td>-0.575926</td>\n",
       "      <td>-7.211844</td>\n",
       "      <td>14.708444</td>\n",
       "      <td>-0.050287</td>\n",
       "      <td>1.231463</td>\n",
       "      <td>0.435285</td>\n",
       "      <td>5.35168</td>\n",
       "      <td>10.891106</td>\n",
       "      <td>-11.571547</td>\n",
       "      <td>-5.697492</td>\n",
       "      <td>13.147513</td>\n",
       "      <td>-8.857483</td>\n",
       "      <td>-2.365073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1976</th>\n",
       "      <td>-0.575926</td>\n",
       "      <td>-7.211844</td>\n",
       "      <td>14.708444</td>\n",
       "      <td>-0.050287</td>\n",
       "      <td>1.231463</td>\n",
       "      <td>0.435285</td>\n",
       "      <td>5.35168</td>\n",
       "      <td>10.891106</td>\n",
       "      <td>-11.571547</td>\n",
       "      <td>-5.697492</td>\n",
       "      <td>13.147513</td>\n",
       "      <td>-8.857483</td>\n",
       "      <td>-2.365073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1977</th>\n",
       "      <td>-0.575926</td>\n",
       "      <td>-7.211844</td>\n",
       "      <td>14.708444</td>\n",
       "      <td>-0.050287</td>\n",
       "      <td>1.231463</td>\n",
       "      <td>0.435285</td>\n",
       "      <td>5.35168</td>\n",
       "      <td>10.891106</td>\n",
       "      <td>-11.571547</td>\n",
       "      <td>-5.697492</td>\n",
       "      <td>13.147513</td>\n",
       "      <td>-8.857483</td>\n",
       "      <td>-2.365073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1978</th>\n",
       "      <td>-0.575926</td>\n",
       "      <td>-7.211844</td>\n",
       "      <td>14.708444</td>\n",
       "      <td>-0.050287</td>\n",
       "      <td>1.231463</td>\n",
       "      <td>0.435285</td>\n",
       "      <td>5.35168</td>\n",
       "      <td>10.891106</td>\n",
       "      <td>-11.571547</td>\n",
       "      <td>-5.697492</td>\n",
       "      <td>13.147513</td>\n",
       "      <td>-8.857483</td>\n",
       "      <td>-2.365073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1979</th>\n",
       "      <td>-0.575926</td>\n",
       "      <td>-7.211844</td>\n",
       "      <td>14.708444</td>\n",
       "      <td>-0.050287</td>\n",
       "      <td>1.231463</td>\n",
       "      <td>0.435285</td>\n",
       "      <td>5.35168</td>\n",
       "      <td>10.891106</td>\n",
       "      <td>-11.571547</td>\n",
       "      <td>-5.697492</td>\n",
       "      <td>13.147513</td>\n",
       "      <td>-8.857483</td>\n",
       "      <td>-2.365073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1980</th>\n",
       "      <td>-0.575926</td>\n",
       "      <td>-7.211844</td>\n",
       "      <td>14.708444</td>\n",
       "      <td>-0.050287</td>\n",
       "      <td>1.231463</td>\n",
       "      <td>0.435285</td>\n",
       "      <td>5.35168</td>\n",
       "      <td>10.891106</td>\n",
       "      <td>-11.571547</td>\n",
       "      <td>-5.697492</td>\n",
       "      <td>13.147513</td>\n",
       "      <td>-8.857483</td>\n",
       "      <td>-2.365073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1981</th>\n",
       "      <td>-0.575926</td>\n",
       "      <td>-7.211844</td>\n",
       "      <td>14.708444</td>\n",
       "      <td>-0.050287</td>\n",
       "      <td>1.231463</td>\n",
       "      <td>0.435285</td>\n",
       "      <td>5.35168</td>\n",
       "      <td>10.891106</td>\n",
       "      <td>-11.571547</td>\n",
       "      <td>-5.697492</td>\n",
       "      <td>13.147513</td>\n",
       "      <td>-8.857483</td>\n",
       "      <td>-2.365073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1982</th>\n",
       "      <td>-0.575926</td>\n",
       "      <td>-7.211844</td>\n",
       "      <td>14.708444</td>\n",
       "      <td>-0.050287</td>\n",
       "      <td>1.231463</td>\n",
       "      <td>0.435285</td>\n",
       "      <td>5.35168</td>\n",
       "      <td>10.891106</td>\n",
       "      <td>-11.571547</td>\n",
       "      <td>-5.697492</td>\n",
       "      <td>13.147513</td>\n",
       "      <td>-8.857483</td>\n",
       "      <td>-2.365073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1983</th>\n",
       "      <td>-0.575926</td>\n",
       "      <td>-7.211844</td>\n",
       "      <td>14.708444</td>\n",
       "      <td>-0.050287</td>\n",
       "      <td>1.231463</td>\n",
       "      <td>0.435285</td>\n",
       "      <td>5.35168</td>\n",
       "      <td>10.891106</td>\n",
       "      <td>-11.571547</td>\n",
       "      <td>-5.697492</td>\n",
       "      <td>13.147513</td>\n",
       "      <td>-8.857483</td>\n",
       "      <td>-2.365073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1984</th>\n",
       "      <td>-0.575926</td>\n",
       "      <td>-7.211844</td>\n",
       "      <td>14.708444</td>\n",
       "      <td>-0.050287</td>\n",
       "      <td>1.231463</td>\n",
       "      <td>0.435285</td>\n",
       "      <td>5.35168</td>\n",
       "      <td>10.891106</td>\n",
       "      <td>-11.571547</td>\n",
       "      <td>-5.697492</td>\n",
       "      <td>13.147513</td>\n",
       "      <td>-8.857483</td>\n",
       "      <td>-2.365073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1985</th>\n",
       "      <td>-0.575926</td>\n",
       "      <td>-7.211844</td>\n",
       "      <td>14.708444</td>\n",
       "      <td>-0.050287</td>\n",
       "      <td>1.231463</td>\n",
       "      <td>0.435285</td>\n",
       "      <td>5.35168</td>\n",
       "      <td>10.891106</td>\n",
       "      <td>-11.571547</td>\n",
       "      <td>-5.697492</td>\n",
       "      <td>13.147513</td>\n",
       "      <td>-8.857483</td>\n",
       "      <td>-2.365073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1986</th>\n",
       "      <td>-0.575926</td>\n",
       "      <td>-7.211844</td>\n",
       "      <td>14.708444</td>\n",
       "      <td>-0.050287</td>\n",
       "      <td>1.231463</td>\n",
       "      <td>0.435285</td>\n",
       "      <td>5.35168</td>\n",
       "      <td>10.891106</td>\n",
       "      <td>-11.571547</td>\n",
       "      <td>-5.697492</td>\n",
       "      <td>13.147513</td>\n",
       "      <td>-8.857483</td>\n",
       "      <td>-2.365073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1987</th>\n",
       "      <td>-0.575926</td>\n",
       "      <td>-7.211844</td>\n",
       "      <td>14.708444</td>\n",
       "      <td>-0.050287</td>\n",
       "      <td>1.231463</td>\n",
       "      <td>0.435285</td>\n",
       "      <td>5.35168</td>\n",
       "      <td>10.891106</td>\n",
       "      <td>-11.571547</td>\n",
       "      <td>-5.697492</td>\n",
       "      <td>13.147513</td>\n",
       "      <td>-8.857483</td>\n",
       "      <td>-2.365073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1988</th>\n",
       "      <td>-0.575926</td>\n",
       "      <td>-7.211844</td>\n",
       "      <td>14.708444</td>\n",
       "      <td>-0.050287</td>\n",
       "      <td>1.231463</td>\n",
       "      <td>0.435285</td>\n",
       "      <td>5.35168</td>\n",
       "      <td>10.891106</td>\n",
       "      <td>-11.571547</td>\n",
       "      <td>-5.697492</td>\n",
       "      <td>13.147513</td>\n",
       "      <td>-8.857483</td>\n",
       "      <td>-2.365073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1989</th>\n",
       "      <td>-0.575926</td>\n",
       "      <td>-7.211844</td>\n",
       "      <td>14.708444</td>\n",
       "      <td>-0.050287</td>\n",
       "      <td>1.231463</td>\n",
       "      <td>0.435285</td>\n",
       "      <td>5.35168</td>\n",
       "      <td>10.891106</td>\n",
       "      <td>-11.571547</td>\n",
       "      <td>-5.697492</td>\n",
       "      <td>13.147513</td>\n",
       "      <td>-8.857483</td>\n",
       "      <td>-2.365073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1990</th>\n",
       "      <td>-0.575926</td>\n",
       "      <td>-7.211844</td>\n",
       "      <td>14.708444</td>\n",
       "      <td>-0.050287</td>\n",
       "      <td>1.231463</td>\n",
       "      <td>0.435285</td>\n",
       "      <td>5.35168</td>\n",
       "      <td>10.891106</td>\n",
       "      <td>-11.571547</td>\n",
       "      <td>-5.697492</td>\n",
       "      <td>13.147513</td>\n",
       "      <td>-8.857483</td>\n",
       "      <td>-2.365073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1991</th>\n",
       "      <td>-0.575926</td>\n",
       "      <td>-7.211844</td>\n",
       "      <td>14.708444</td>\n",
       "      <td>-0.050287</td>\n",
       "      <td>1.231463</td>\n",
       "      <td>0.435285</td>\n",
       "      <td>5.35168</td>\n",
       "      <td>10.891106</td>\n",
       "      <td>-11.571547</td>\n",
       "      <td>-5.697492</td>\n",
       "      <td>13.147513</td>\n",
       "      <td>-8.857483</td>\n",
       "      <td>-2.365073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1992</th>\n",
       "      <td>-0.575926</td>\n",
       "      <td>-7.211844</td>\n",
       "      <td>14.708444</td>\n",
       "      <td>-0.050287</td>\n",
       "      <td>1.231463</td>\n",
       "      <td>0.435285</td>\n",
       "      <td>5.35168</td>\n",
       "      <td>10.891106</td>\n",
       "      <td>-11.571547</td>\n",
       "      <td>-5.697492</td>\n",
       "      <td>13.147513</td>\n",
       "      <td>-8.857483</td>\n",
       "      <td>-2.365073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1993</th>\n",
       "      <td>-0.575926</td>\n",
       "      <td>-7.211844</td>\n",
       "      <td>14.708444</td>\n",
       "      <td>-0.050287</td>\n",
       "      <td>1.231463</td>\n",
       "      <td>0.435285</td>\n",
       "      <td>5.35168</td>\n",
       "      <td>10.891106</td>\n",
       "      <td>-11.571547</td>\n",
       "      <td>-5.697492</td>\n",
       "      <td>13.147513</td>\n",
       "      <td>-8.857483</td>\n",
       "      <td>-2.365073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1994</th>\n",
       "      <td>-0.575926</td>\n",
       "      <td>-7.211844</td>\n",
       "      <td>14.708444</td>\n",
       "      <td>-0.050287</td>\n",
       "      <td>1.231463</td>\n",
       "      <td>0.435285</td>\n",
       "      <td>5.35168</td>\n",
       "      <td>10.891106</td>\n",
       "      <td>-11.571547</td>\n",
       "      <td>-5.697492</td>\n",
       "      <td>13.147513</td>\n",
       "      <td>-8.857483</td>\n",
       "      <td>-2.365073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>-0.575926</td>\n",
       "      <td>-7.211844</td>\n",
       "      <td>14.708444</td>\n",
       "      <td>-0.050287</td>\n",
       "      <td>1.231463</td>\n",
       "      <td>0.435285</td>\n",
       "      <td>5.35168</td>\n",
       "      <td>10.891106</td>\n",
       "      <td>-11.571547</td>\n",
       "      <td>-5.697492</td>\n",
       "      <td>13.147513</td>\n",
       "      <td>-8.857483</td>\n",
       "      <td>-2.365073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>-0.575926</td>\n",
       "      <td>-7.211844</td>\n",
       "      <td>14.708444</td>\n",
       "      <td>-0.050287</td>\n",
       "      <td>1.231463</td>\n",
       "      <td>0.435285</td>\n",
       "      <td>5.35168</td>\n",
       "      <td>10.891106</td>\n",
       "      <td>-11.571547</td>\n",
       "      <td>-5.697492</td>\n",
       "      <td>13.147513</td>\n",
       "      <td>-8.857483</td>\n",
       "      <td>-2.365073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>-0.575926</td>\n",
       "      <td>-7.211844</td>\n",
       "      <td>14.708444</td>\n",
       "      <td>-0.050287</td>\n",
       "      <td>1.231463</td>\n",
       "      <td>0.435285</td>\n",
       "      <td>5.35168</td>\n",
       "      <td>10.891106</td>\n",
       "      <td>-11.571547</td>\n",
       "      <td>-5.697492</td>\n",
       "      <td>13.147513</td>\n",
       "      <td>-8.857483</td>\n",
       "      <td>-2.365073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>-0.575926</td>\n",
       "      <td>-7.211844</td>\n",
       "      <td>14.708444</td>\n",
       "      <td>-0.050287</td>\n",
       "      <td>1.231463</td>\n",
       "      <td>0.435285</td>\n",
       "      <td>5.35168</td>\n",
       "      <td>10.891106</td>\n",
       "      <td>-11.571547</td>\n",
       "      <td>-5.697492</td>\n",
       "      <td>13.147513</td>\n",
       "      <td>-8.857483</td>\n",
       "      <td>-2.365073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>-0.575926</td>\n",
       "      <td>-7.211844</td>\n",
       "      <td>14.708444</td>\n",
       "      <td>-0.050287</td>\n",
       "      <td>1.231463</td>\n",
       "      <td>0.435285</td>\n",
       "      <td>5.35168</td>\n",
       "      <td>10.891106</td>\n",
       "      <td>-11.571547</td>\n",
       "      <td>-5.697492</td>\n",
       "      <td>13.147513</td>\n",
       "      <td>-8.857483</td>\n",
       "      <td>-2.365073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000</th>\n",
       "      <td>-0.575926</td>\n",
       "      <td>-7.211844</td>\n",
       "      <td>14.708444</td>\n",
       "      <td>-0.050287</td>\n",
       "      <td>1.231463</td>\n",
       "      <td>0.435285</td>\n",
       "      <td>5.35168</td>\n",
       "      <td>10.891106</td>\n",
       "      <td>-11.571547</td>\n",
       "      <td>-5.697492</td>\n",
       "      <td>13.147513</td>\n",
       "      <td>-8.857483</td>\n",
       "      <td>-2.365073</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2001 rows  13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1          2         3         4         5        6   \\\n",
       "0    -0.575926 -7.211844  14.708444 -0.050287  1.231463  0.435285  5.35168   \n",
       "1    -0.575926 -7.211844  14.708444 -0.050287  1.231463  0.435285  5.35168   \n",
       "2    -0.575926 -7.211844  14.708444 -0.050287  1.231463  0.435285  5.35168   \n",
       "3    -0.575926 -7.211844  14.708444 -0.050287  1.231463  0.435285  5.35168   \n",
       "4    -0.575926 -7.211844  14.708444 -0.050287  1.231463  0.435285  5.35168   \n",
       "5    -0.575926 -7.211844  14.708444 -0.050287  1.231463  0.435285  5.35168   \n",
       "6    -0.575926 -7.211844  14.708444 -0.050287  1.231463  0.435285  5.35168   \n",
       "7    -0.575926 -7.211844  14.708444 -0.050287  1.231463  0.435285  5.35168   \n",
       "8    -0.575926 -7.211844  14.708444 -0.050287  1.231463  0.435285  5.35168   \n",
       "9    -0.575926 -7.211844  14.708444 -0.050287  1.231463  0.435285  5.35168   \n",
       "10   -0.575926 -7.211844  14.708444 -0.050287  1.231463  0.435285  5.35168   \n",
       "11   -0.575926 -7.211844  14.708444 -0.050287  1.231463  0.435285  5.35168   \n",
       "12   -0.575926 -7.211844  14.708444 -0.050287  1.231463  0.435285  5.35168   \n",
       "13   -0.575926 -7.211844  14.708444 -0.050287  1.231463  0.435285  5.35168   \n",
       "14   -0.575926 -7.211844  14.708444 -0.050287  1.231463  0.435285  5.35168   \n",
       "15   -0.575926 -7.211844  14.708444 -0.050287  1.231463  0.435285  5.35168   \n",
       "16   -0.575926 -7.211844  14.708444 -0.050287  1.231463  0.435285  5.35168   \n",
       "17   -0.575926 -7.211844  14.708444 -0.050287  1.231463  0.435285  5.35168   \n",
       "18   -0.575926 -7.211844  14.708444 -0.050287  1.231463  0.435285  5.35168   \n",
       "19   -0.575926 -7.211844  14.708444 -0.050287  1.231463  0.435285  5.35168   \n",
       "20   -0.575926 -7.211844  14.708444 -0.050287  1.231463  0.435285  5.35168   \n",
       "21   -0.575926 -7.211844  14.708444 -0.050287  1.231463  0.435285  5.35168   \n",
       "22   -0.575926 -7.211844  14.708444 -0.050287  1.231463  0.435285  5.35168   \n",
       "23   -0.575926 -7.211844  14.708444 -0.050287  1.231463  0.435285  5.35168   \n",
       "24   -0.575926 -7.211844  14.708444 -0.050287  1.231463  0.435285  5.35168   \n",
       "25   -0.575926 -7.211844  14.708444 -0.050287  1.231463  0.435285  5.35168   \n",
       "26   -0.575926 -7.211844  14.708444 -0.050287  1.231463  0.435285  5.35168   \n",
       "27   -0.575926 -7.211844  14.708444 -0.050287  1.231463  0.435285  5.35168   \n",
       "28   -0.575926 -7.211844  14.708444 -0.050287  1.231463  0.435285  5.35168   \n",
       "29   -0.575926 -7.211844  14.708444 -0.050287  1.231463  0.435285  5.35168   \n",
       "...        ...       ...        ...       ...       ...       ...      ...   \n",
       "1971 -0.575926 -7.211844  14.708444 -0.050287  1.231463  0.435285  5.35168   \n",
       "1972 -0.575926 -7.211844  14.708444 -0.050287  1.231463  0.435285  5.35168   \n",
       "1973 -0.575926 -7.211844  14.708444 -0.050287  1.231463  0.435285  5.35168   \n",
       "1974 -0.575926 -7.211844  14.708444 -0.050287  1.231463  0.435285  5.35168   \n",
       "1975 -0.575926 -7.211844  14.708444 -0.050287  1.231463  0.435285  5.35168   \n",
       "1976 -0.575926 -7.211844  14.708444 -0.050287  1.231463  0.435285  5.35168   \n",
       "1977 -0.575926 -7.211844  14.708444 -0.050287  1.231463  0.435285  5.35168   \n",
       "1978 -0.575926 -7.211844  14.708444 -0.050287  1.231463  0.435285  5.35168   \n",
       "1979 -0.575926 -7.211844  14.708444 -0.050287  1.231463  0.435285  5.35168   \n",
       "1980 -0.575926 -7.211844  14.708444 -0.050287  1.231463  0.435285  5.35168   \n",
       "1981 -0.575926 -7.211844  14.708444 -0.050287  1.231463  0.435285  5.35168   \n",
       "1982 -0.575926 -7.211844  14.708444 -0.050287  1.231463  0.435285  5.35168   \n",
       "1983 -0.575926 -7.211844  14.708444 -0.050287  1.231463  0.435285  5.35168   \n",
       "1984 -0.575926 -7.211844  14.708444 -0.050287  1.231463  0.435285  5.35168   \n",
       "1985 -0.575926 -7.211844  14.708444 -0.050287  1.231463  0.435285  5.35168   \n",
       "1986 -0.575926 -7.211844  14.708444 -0.050287  1.231463  0.435285  5.35168   \n",
       "1987 -0.575926 -7.211844  14.708444 -0.050287  1.231463  0.435285  5.35168   \n",
       "1988 -0.575926 -7.211844  14.708444 -0.050287  1.231463  0.435285  5.35168   \n",
       "1989 -0.575926 -7.211844  14.708444 -0.050287  1.231463  0.435285  5.35168   \n",
       "1990 -0.575926 -7.211844  14.708444 -0.050287  1.231463  0.435285  5.35168   \n",
       "1991 -0.575926 -7.211844  14.708444 -0.050287  1.231463  0.435285  5.35168   \n",
       "1992 -0.575926 -7.211844  14.708444 -0.050287  1.231463  0.435285  5.35168   \n",
       "1993 -0.575926 -7.211844  14.708444 -0.050287  1.231463  0.435285  5.35168   \n",
       "1994 -0.575926 -7.211844  14.708444 -0.050287  1.231463  0.435285  5.35168   \n",
       "1995 -0.575926 -7.211844  14.708444 -0.050287  1.231463  0.435285  5.35168   \n",
       "1996 -0.575926 -7.211844  14.708444 -0.050287  1.231463  0.435285  5.35168   \n",
       "1997 -0.575926 -7.211844  14.708444 -0.050287  1.231463  0.435285  5.35168   \n",
       "1998 -0.575926 -7.211844  14.708444 -0.050287  1.231463  0.435285  5.35168   \n",
       "1999 -0.575926 -7.211844  14.708444 -0.050287  1.231463  0.435285  5.35168   \n",
       "2000 -0.575926 -7.211844  14.708444 -0.050287  1.231463  0.435285  5.35168   \n",
       "\n",
       "             7          8         9          10        11        12  \n",
       "0     10.891106 -11.571547 -5.697492  13.147513 -8.857483 -2.365073  \n",
       "1     10.891106 -11.571547 -5.697492  13.147513 -8.857483 -2.365073  \n",
       "2     10.891106 -11.571547 -5.697492  13.147513 -8.857483 -2.365073  \n",
       "3     10.891106 -11.571547 -5.697492  13.147513 -8.857483 -2.365073  \n",
       "4     10.891106 -11.571547 -5.697492  13.147513 -8.857483 -2.365073  \n",
       "5     10.891106 -11.571547 -5.697492  13.147513 -8.857483 -2.365073  \n",
       "6     10.891106 -11.571547 -5.697492  13.147513 -8.857483 -2.365073  \n",
       "7     10.891106 -11.571547 -5.697492  13.147513 -8.857483 -2.365073  \n",
       "8     10.891106 -11.571547 -5.697492  13.147513 -8.857483 -2.365073  \n",
       "9     10.891106 -11.571547 -5.697492  13.147513 -8.857483 -2.365073  \n",
       "10    10.891106 -11.571547 -5.697492  13.147513 -8.857483 -2.365073  \n",
       "11    10.891106 -11.571547 -5.697492  13.147513 -8.857483 -2.365073  \n",
       "12    10.891106 -11.571547 -5.697492  13.147513 -8.857483 -2.365073  \n",
       "13    10.891106 -11.571547 -5.697492  13.147513 -8.857483 -2.365073  \n",
       "14    10.891106 -11.571547 -5.697492  13.147513 -8.857483 -2.365073  \n",
       "15    10.891106 -11.571547 -5.697492  13.147513 -8.857483 -2.365073  \n",
       "16    10.891106 -11.571547 -5.697492  13.147513 -8.857483 -2.365073  \n",
       "17    10.891106 -11.571547 -5.697492  13.147513 -8.857483 -2.365073  \n",
       "18    10.891106 -11.571547 -5.697492  13.147513 -8.857483 -2.365073  \n",
       "19    10.891106 -11.571547 -5.697492  13.147513 -8.857483 -2.365073  \n",
       "20    10.891106 -11.571547 -5.697492  13.147513 -8.857483 -2.365073  \n",
       "21    10.891106 -11.571547 -5.697492  13.147513 -8.857483 -2.365073  \n",
       "22    10.891106 -11.571547 -5.697492  13.147513 -8.857483 -2.365073  \n",
       "23    10.891106 -11.571547 -5.697492  13.147513 -8.857483 -2.365073  \n",
       "24    10.891106 -11.571547 -5.697492  13.147513 -8.857483 -2.365073  \n",
       "25    10.891106 -11.571547 -5.697492  13.147513 -8.857483 -2.365073  \n",
       "26    10.891106 -11.571547 -5.697492  13.147513 -8.857483 -2.365073  \n",
       "27    10.891106 -11.571547 -5.697492  13.147513 -8.857483 -2.365073  \n",
       "28    10.891106 -11.571547 -5.697492  13.147513 -8.857483 -2.365073  \n",
       "29    10.891106 -11.571547 -5.697492  13.147513 -8.857483 -2.365073  \n",
       "...         ...        ...       ...        ...       ...       ...  \n",
       "1971  10.891106 -11.571547 -5.697492  13.147513 -8.857483 -2.365073  \n",
       "1972  10.891106 -11.571547 -5.697492  13.147513 -8.857483 -2.365073  \n",
       "1973  10.891106 -11.571547 -5.697492  13.147513 -8.857483 -2.365073  \n",
       "1974  10.891106 -11.571547 -5.697492  13.147513 -8.857483 -2.365073  \n",
       "1975  10.891106 -11.571547 -5.697492  13.147513 -8.857483 -2.365073  \n",
       "1976  10.891106 -11.571547 -5.697492  13.147513 -8.857483 -2.365073  \n",
       "1977  10.891106 -11.571547 -5.697492  13.147513 -8.857483 -2.365073  \n",
       "1978  10.891106 -11.571547 -5.697492  13.147513 -8.857483 -2.365073  \n",
       "1979  10.891106 -11.571547 -5.697492  13.147513 -8.857483 -2.365073  \n",
       "1980  10.891106 -11.571547 -5.697492  13.147513 -8.857483 -2.365073  \n",
       "1981  10.891106 -11.571547 -5.697492  13.147513 -8.857483 -2.365073  \n",
       "1982  10.891106 -11.571547 -5.697492  13.147513 -8.857483 -2.365073  \n",
       "1983  10.891106 -11.571547 -5.697492  13.147513 -8.857483 -2.365073  \n",
       "1984  10.891106 -11.571547 -5.697492  13.147513 -8.857483 -2.365073  \n",
       "1985  10.891106 -11.571547 -5.697492  13.147513 -8.857483 -2.365073  \n",
       "1986  10.891106 -11.571547 -5.697492  13.147513 -8.857483 -2.365073  \n",
       "1987  10.891106 -11.571547 -5.697492  13.147513 -8.857483 -2.365073  \n",
       "1988  10.891106 -11.571547 -5.697492  13.147513 -8.857483 -2.365073  \n",
       "1989  10.891106 -11.571547 -5.697492  13.147513 -8.857483 -2.365073  \n",
       "1990  10.891106 -11.571547 -5.697492  13.147513 -8.857483 -2.365073  \n",
       "1991  10.891106 -11.571547 -5.697492  13.147513 -8.857483 -2.365073  \n",
       "1992  10.891106 -11.571547 -5.697492  13.147513 -8.857483 -2.365073  \n",
       "1993  10.891106 -11.571547 -5.697492  13.147513 -8.857483 -2.365073  \n",
       "1994  10.891106 -11.571547 -5.697492  13.147513 -8.857483 -2.365073  \n",
       "1995  10.891106 -11.571547 -5.697492  13.147513 -8.857483 -2.365073  \n",
       "1996  10.891106 -11.571547 -5.697492  13.147513 -8.857483 -2.365073  \n",
       "1997  10.891106 -11.571547 -5.697492  13.147513 -8.857483 -2.365073  \n",
       "1998  10.891106 -11.571547 -5.697492  13.147513 -8.857483 -2.365073  \n",
       "1999  10.891106 -11.571547 -5.697492  13.147513 -8.857483 -2.365073  \n",
       "2000  10.891106 -11.571547 -5.697492  13.147513 -8.857483 -2.365073  \n",
       "\n",
       "[2001 rows x 13 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(weights_over_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'numpy.ndarray'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m/opt/conda/envs/learn-env/lib/python3.6/site-packages/pandas/core/arrays/categorical.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, values, categories, ordered, dtype, fastpath)\u001b[0m\n\u001b[1;32m    344\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m                 \u001b[0mcodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcategories\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfactorize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msort\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/learn-env/lib/python3.6/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    177\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/learn-env/lib/python3.6/site-packages/pandas/core/algorithms.py\u001b[0m in \u001b[0;36mfactorize\u001b[0;34m(values, sort, order, na_sentinel, size_hint)\u001b[0m\n\u001b[1;32m    629\u001b[0m                                            \u001b[0msize_hint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msize_hint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 630\u001b[0;31m                                            na_value=na_value)\n\u001b[0m\u001b[1;32m    631\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/learn-env/lib/python3.6/site-packages/pandas/core/algorithms.py\u001b[0m in \u001b[0;36m_factorize_array\u001b[0;34m(values, na_sentinel, size_hint, na_value)\u001b[0m\n\u001b[1;32m    475\u001b[0m     labels = table.get_labels(values, uniques, 0, na_sentinel,\n\u001b[0;32m--> 476\u001b[0;31m                               na_value=na_value)\n\u001b[0m\u001b[1;32m    477\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_labels\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'numpy.ndarray'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-808df23ddefc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtime_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10005\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0msns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlineplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights_over_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/envs/learn-env/lib/python3.6/site-packages/seaborn/relational.py\u001b[0m in \u001b[0;36mlineplot\u001b[0;34m(x, y, hue, size, style, data, palette, hue_order, hue_norm, sizes, size_order, size_norm, dashes, markers, style_order, units, estimator, ci, n_boot, sort, err_style, err_kws, legend, ax, **kwargs)\u001b[0m\n\u001b[1;32m   1082\u001b[0m         \u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgca\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1083\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1084\u001b[0;31m     \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1085\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1086\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/learn-env/lib/python3.6/site-packages/seaborn/relational.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(self, ax, kws)\u001b[0m\n\u001b[1;32m    764\u001b[0m         \u001b[0;31m# Loop over the semantic subsets and draw a line for each\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    765\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 766\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0msemantics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubset_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    767\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    768\u001b[0m             \u001b[0mhue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstyle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msemantics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/learn-env/lib/python3.6/site-packages/seaborn/relational.py\u001b[0m in \u001b[0;36msubset_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    330\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m                 \u001b[0msubset_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msort_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubset_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"units\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"x\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"y\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munits\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/learn-env/lib/python3.6/site-packages/seaborn/utils.py\u001b[0m in \u001b[0;36msort_df\u001b[0;34m(df, *args, **kwargs)\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;34m\"\"\"Wrapper to handle different pandas sorting API pre/post 0.17.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/learn-env/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36msort_values\u001b[0;34m(self, by, axis, ascending, inplace, kind, na_position)\u001b[0m\n\u001b[1;32m   4412\u001b[0m                 \u001b[0mkeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4413\u001b[0m             indexer = lexsort_indexer(keys, orders=ascending,\n\u001b[0;32m-> 4414\u001b[0;31m                                       na_position=na_position)\n\u001b[0m\u001b[1;32m   4415\u001b[0m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_ensure_platform_int\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4416\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/learn-env/lib/python3.6/site-packages/pandas/core/sorting.py\u001b[0m in \u001b[0;36mlexsort_indexer\u001b[0;34m(keys, orders, na_position)\u001b[0m\n\u001b[1;32m    205\u001b[0m         \u001b[0;31m# create the Categorical\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 207\u001b[0;31m             \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCategorical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mordered\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    208\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mna_position\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'last'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'first'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/learn-env/lib/python3.6/site-packages/pandas/core/arrays/categorical.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, values, categories, ordered, dtype, fastpath)\u001b[0m\n\u001b[1;32m    345\u001b[0m                 \u001b[0mcodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcategories\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfactorize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msort\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m                 \u001b[0mcodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcategories\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfactorize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msort\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    348\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mordered\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m                     \u001b[0;31m# raise, as we don't have a sortable data structure and so\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/learn-env/lib/python3.6/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    176\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_deprecate_kwarg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/learn-env/lib/python3.6/site-packages/pandas/core/algorithms.py\u001b[0m in \u001b[0;36mfactorize\u001b[0;34m(values, sort, order, na_sentinel, size_hint)\u001b[0m\n\u001b[1;32m    628\u001b[0m                                            \u001b[0mna_sentinel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mna_sentinel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m                                            \u001b[0msize_hint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msize_hint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 630\u001b[0;31m                                            na_value=na_value)\n\u001b[0m\u001b[1;32m    631\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msort\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/learn-env/lib/python3.6/site-packages/pandas/core/algorithms.py\u001b[0m in \u001b[0;36m_factorize_array\u001b[0;34m(values, na_sentinel, size_hint, na_value)\u001b[0m\n\u001b[1;32m    474\u001b[0m     \u001b[0muniques\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvec_klass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    475\u001b[0m     labels = table.get_labels(values, uniques, 0, na_sentinel,\n\u001b[0;32m--> 476\u001b[0;31m                               na_value=na_value)\n\u001b[0m\u001b[1;32m    477\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    478\u001b[0m     \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_ensure_platform_int\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_labels\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'numpy.ndarray'"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "time_x = np.arange(0, 10005, 5)\n",
    "#sns.lineplot(time_x, weights_over_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0    165\n",
      "0.0    138\n",
      "Name: target, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ -0.57592597,  -7.21184424,  14.70844387,  -0.05028691,\n",
       "         1.23146282,   0.43528515,   5.35167991,  10.89110559,\n",
       "       -11.57154703,  -5.69749248,  13.14751269,  -8.85748332,\n",
       "        -2.36507274])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_weights = grad_desc(X, y)\n",
    "final_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sci-kit learn\n",
    "\n",
    "For comparison, import sci-kit learn's standard LogisticRegression function. Initialize a regression object with **no intercept** and with **C=1e16** or another very high number. The reason is as follows: our implementation has not used an intercept, and we have not performed any regularization such as Lasso or Ridge (sci-kit learn uses l2 by default). The high value of C will essentially negate this.\n",
    "\n",
    "After initializing a regression object, fit it to X and y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "log_reg = LogisticRegression(C=)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare the models\n",
    "\n",
    "Compare the coefficient weights of your model to that generated by sci-kit learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Level - Up\n",
    "\n",
    "Update the gradient descent algorithm to also return the prediction error after each iteration. Then rerun the algorithm and create a graph displaying the prediction errors versus the iteration number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Resources\n",
    "\n",
    "If you want to see more of the mathematics behind the gradient derivation above, check out section 4.4.1 from the Elements of Statistical Learning which can be found here: https://web.stanford.edu/~hastie/ElemStatLearn//."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Congratulations! You just coded logistic regression from the ground up using NumPy! With this, you should have a fairly deep understanding of logistic regression and how the algorithm works! In the upcoming labs, we'll continue to explore this from a few more angles, plotting our data along with the decision boundary for our predictions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
